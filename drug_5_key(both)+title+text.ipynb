{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd31a762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score,plot_confusion_matrix,multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d1752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras import Model\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.layers import Bidirectional, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714a588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      title  body  title_keyphrase  body_keyphrase\n",
      "label_classification                                              \n",
      "A-Recovery               20    20               20              20\n",
      "Addicted                 20    20               20              20\n",
      "E-Recovery               20    20               20              20\n",
      "M-Recovery               20    20               20              20\n",
      "Others                   20    20               20              20\n",
      "25 1000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"DASH_2020_Drug_Data_train20_keyphrase.csv\", usecols=['title','body','label_classification','body_keyphrase', 'title_keyphrase'])\n",
    "group = df.groupby(\"label_classification\")\n",
    "print(group.count())\n",
    "\n",
    "df['label_classification'][df['label_classification']=='Addicted'] = 1\n",
    "df['label_classification'][df['label_classification']=='E-Recovery'] = 2\n",
    "df['label_classification'][df['label_classification']=='M-Recovery'] = 3\n",
    "df['label_classification'][df['label_classification']=='A-Recovery'] = 4\n",
    "df['label_classification'][df['label_classification']=='Others'] = 5\n",
    "\n",
    "# df=df.astype({'body': str, 'label_classification': 'int'})\n",
    "\n",
    "df1 = df.loc[df['label_classification']==1][:5]\n",
    "df2 = df.loc[df['label_classification']==2][:5]\n",
    "df3 = df.loc[df['label_classification']==3][:5]\n",
    "df4 = df.loc[df['label_classification']==4][:5]\n",
    "df5 = df.loc[df['label_classification']==5][:5]\n",
    "\n",
    "frames = [df1,df2,df3,df4,df5]\n",
    "train = pd.concat(frames)\n",
    "train= train.reset_index(drop=True).dropna()\n",
    "\n",
    "test = pd.read_csv(\"DASH_2020_Drug_Data_test_keyphrase.csv\", usecols=['title','body','label_classification','body_keyphrase','title_keyphrase'])\n",
    "test['label_classification'][test['label_classification']=='Addicted'] = 1\n",
    "test['label_classification'][test['label_classification']=='E-Recovery'] = 2\n",
    "test['label_classification'][test['label_classification']=='M-Recovery'] = 3\n",
    "test['label_classification'][test['label_classification']=='A-Recovery'] = 4\n",
    "test['label_classification'][test['label_classification']=='Others'] = 5\n",
    "\n",
    "print(len(train),len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796dba00",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for i,row in train.iterrows():\n",
    "    words1 = re.findall('[(](.*?)[,]', train.loc[i,'title_keyphrase'])\n",
    "    words1 = [word[1:-1] for word in words1] if len(words1)<3 else [word[1:-1] for word in words1[:3]]\n",
    "    cur_keys1 = '' if not words1 else ', '.join([word for word in words1])+'. '\n",
    "    words2 = re.findall('[(](.*?)[,]', train.loc[i,'body_keyphrase'])\n",
    "    words2 = [word[1:-1] for word in words2] if len(words2)<3 else [word[1:-1] for word in words2[:3]]\n",
    "    cur_keys2 = '' if not words2 else ', '.join([word for word in words2])+'. '\n",
    "    train.loc[i,'body'] = train.loc[i,'title']+'. '+cur_keys1+cur_keys2+train.loc[i,'body'] if len(train.loc[i,'title'])>0 else cur_keys1+cur_keys2+train.loc[i,'body']\n",
    "\n",
    "for i,row in test.iterrows():\n",
    "    words1 = re.findall('[(](.*?)[,]', test.loc[i,'title_keyphrase'])\n",
    "    words1 = [word[1:-1] for word in words1] if len(words1)<3 else [word[1:-1] for word in words1[:3]]\n",
    "    cur_keys1 = '' if not words1 else ', '.join([word for word in words1])+'. '\n",
    "    words2 = re.findall('[(](.*?)[,]', test.loc[i,'body_keyphrase'])\n",
    "    words2 = [word[1:-1] for word in words2] if len(words2)<3 else [word[1:-1] for word in words2[:3]]\n",
    "    cur_keys2 = '' if not words2 else ', '.join([word for word in words2])+'. '\n",
    "    test.loc[i,'body'] = test.loc[i,'title']+'. '+cur_keys1+cur_keys2+test.loc[i,'body'] if len(test.loc[i,'title'])>0 else cur_keys1+cur_keys2+test.loc[i,'body']\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b302de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-large/4'\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c93301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "norm_layer (Lambda)          (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 157,120\n",
      "Trainable params: 156,608\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_text1 = Input(shape=(512,))\n",
    "x = Dense(256, activation='relu')(input_text1)\n",
    "x = Dropout(0.4)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "dense_layer = Dense(128, name='dense_layer')(x)\n",
    "norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n",
    "\n",
    "model=Model(inputs=[input_text1], outputs=norm_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a088bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer\n",
    "\n",
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(512,))\n",
    "in_p = Input(shape=(512,))\n",
    "in_n = Input(shape=(512,))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "# The nn4_small model instance is shared (Siamese network)\n",
    "emb_a = model(in_a)\n",
    "emb_p = model(in_p)\n",
    "emb_n = model(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.4, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb91d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# creating the necessary datastructures for selcting triplets\n",
    "unique_train_label=np.array(train['label_classification'].unique().tolist())\n",
    "labels_train=np.array(train['label_classification'].tolist())\n",
    "map_train_label_indices = {label: np.flatnonzero(labels_train == label) for label in unique_train_label}\n",
    "\n",
    "def get_triplets(unique_train_label,map_train_label_indices):\n",
    "    label_l, label_r = np.random.choice(unique_train_label, 2, replace=False)\n",
    "    a, p = np.random.choice(map_train_label_indices[label_l],2, replace=False)\n",
    "    n = np.random.choice(map_train_label_indices[label_r])\n",
    "    return a, p, n\n",
    "\n",
    "def get_triplets_batch(k,train_set,unique_train_label,map_train_label_indices,embed):\n",
    "\n",
    "    while True:\n",
    "        idxs_a, idxs_p, idxs_n = [], [], []\n",
    "        for _ in range(k):\n",
    "            a, p, n = get_triplets(unique_train_label,map_train_label_indices)\n",
    "            idxs_a.append(a)\n",
    "            idxs_p.append(p)\n",
    "            idxs_n.append(n)\n",
    "\n",
    "        a=train_set.iloc[idxs_a].values.tolist()\n",
    "        b=train_set.iloc[idxs_p].values.tolist()\n",
    "        c=train_set.iloc[idxs_n].values.tolist()\n",
    "\n",
    "        a = embed(a)\n",
    "        p = embed(b)\n",
    "        n = embed(c)\n",
    "        # return train_set[idxs_a], train_set[idxs_p], train_set[idxs_n]\n",
    "        yield [a,p,n], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67494cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn4_small2_train.compile(loss=None, optimizer='adam')\n",
    "history = nn4_small2_train.fit(get_triplets_batch(32,train['body'],unique_train_label,map_train_label_indices,embed), epochs=50,steps_per_epoch=10)\n",
    "nn4_small2_train.save('drug_few-shot5_key(both)+title+text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bffcfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 46.37it/s]\n",
      "100%|██████████| 1000/1000 [00:20<00:00, 48.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy = 0.517, SVM accuracy = 0.519\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# test=test.sample(n=300,random_state=200).reset_index(drop=True)\n",
    "#getting the embeddings from the model\n",
    "encoded = []\n",
    "for val in tqdm(np.array(train['body'].values.tolist())):\n",
    "    encoded.append(embed([val])['outputs'][0])\n",
    "X_train_input = {'input_1':tf.convert_to_tensor(encoded)}\n",
    "\n",
    "encoded = []\n",
    "for val in tqdm(np.array(test['body'].values.tolist())):\n",
    "    encoded.append(embed([val])['outputs'][0])\n",
    "X_test_input = {'input_1':tf.convert_to_tensor(encoded)}\n",
    "\n",
    "X_train = model.predict(X_train_input)\n",
    "X_test = model.predict(X_test_input)\n",
    "\n",
    "y_train = np.array(train['label_classification'].values.tolist())\n",
    "y_test = np.array(test['label_classification'].values.tolist())\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "svc = LinearSVC()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "\n",
    "print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e808e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5104    0.7350    0.6025       200\n",
      "           2     0.5000    0.3950    0.4413       200\n",
      "           3     0.3807    0.4150    0.3971       200\n",
      "           4     0.5357    0.5250    0.5303       200\n",
      "           5     0.7357    0.5150    0.6059       200\n",
      "\n",
      "    accuracy                         0.5170      1000\n",
      "   macro avg     0.5325    0.5170    0.5154      1000\n",
      "weighted avg     0.5325    0.5170    0.5154      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(list(y_test), list(y_pred_knn), digits=4))\n",
    "with open('drug_result_few-shot5_key(both)+title+text.txt','w') as f:\n",
    "    f.write('KNN accuracy: '+str(acc_knn)+'\\n'+'SVM accuracy: '+str(acc_svc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
